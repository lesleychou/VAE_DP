{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eca4994",
   "metadata": {},
   "source": [
    "\n",
    "# SUPPORT Notebook\n",
    "\n",
    "This notebook runs through investigations on the open access SUPPORT dataset.\n",
    "\n",
    "For users who do not have lots of computational resources or do not have access to MIMIC-III then this notebook should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b724eb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Import Libraries -------- #\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# VAE is in other folder as well as opacus adapted library\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# Opacus support for differential privacy\n",
    "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
    "\n",
    "# For the SUPPORT dataset\n",
    "from pycox.datasets import support\n",
    "\n",
    "# For VAE dataset formatting\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# VAE functions\n",
    "from VAE import Decoder, Encoder, VAE\n",
    "\n",
    "# Utility file contains all functions required to run notebook\n",
    "from utils import (\n",
    "    set_seed,\n",
    "    support_pre_proc,\n",
    "    plot_elbo,\n",
    "    plot_likelihood_breakdown,\n",
    "    plot_variable_distributions,\n",
    "    reverse_transformers,\n",
    ")\n",
    "from metrics import distribution_metrics, privacy_metrics\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # We suppress warnings to avoid SDMETRICS throwing unique synthetic data warnings (i.e.\n",
    "# data in synthetic set is not in the real data set) as well as SKLEARN throwing convergence warnings (pre-processing uses\n",
    "# GMM from sklearn and this throws non convergence warnings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3b1602",
   "metadata": {},
   "source": [
    "## Data Loading & Column Definitions\n",
    "\n",
    "First we load in the SUPPORT dataset from pycox datasets. Then we define the continuous and categorical columns in that dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7afb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "\n",
    "# Load in the support data\n",
    "data_supp = support.read_df()\n",
    "\n",
    "# Column Definitions\n",
    "original_continuous_columns = [\"duration\"] + [f\"x{i}\" for i in range(7, 15)]\n",
    "original_categorical_columns = [\"event\"] + [f\"x{i}\" for i in range(1, 7)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81156c9b",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "\n",
    "Data can be pre-processed in 2 ways. Either we use <b>\"standard\"</b> option which performs a standard scaler on continuous variables - This has known limitations as:\n",
    "\n",
    "- Data in tables is usually non-gaussian and SynthVAE implements a gaussian loss, so this will perform worse unless the data is KNOWN to follow a gaussian distribution already.\n",
    "\n",
    "Or we use the second option of <b>\"GMM\"</b>. This performs a variational gaussian mixture model to scale the data & transform it to a gaussian distribution. We use a maximum number of clusters of 10 but the variational method will select the best number of clusters for that continuous variable. This also has known limitations:\n",
    "\n",
    "- 10 Clusters is arbitrary and may not be enough for certain variables.\n",
    "- We are fitting a model to transform the data and hence we are approximating before model is trained. This will lose fidelity as the distribution will not be transformed perfectly.\n",
    "\n",
    "SUPPORT is a limited dataset as it has no missingness (which our model currently does NOT handle) and it has no datetime columns or other data types. Be wary drawing any conclusions from this set due to these constraints as well as the dataset size. Testing/training new models with this set could be useful but conclusive results should be tested on other sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e663fe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Data Pre-Processing -------- #\n",
    "\n",
    "pre_proc_method = \"standard\"\n",
    "\n",
    "(\n",
    "    x_train,\n",
    "    data_supp,\n",
    "    reordered_dataframe_columns,\n",
    "    continuous_transformers,\n",
    "    categorical_transformers,\n",
    "    num_categories,\n",
    "    num_continuous,\n",
    ") = support_pre_proc(data_supp=data_supp, pre_proc_method=pre_proc_method)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe28857",
   "metadata": {},
   "source": [
    "## Creation & Training of VAE\n",
    "\n",
    "We can adapt certain parameters of the model e.g. batch size, latent dimension size etc. This model implements early stopping and these values can be adapted.\n",
    "\n",
    "We can also activate differential privacy by implementing dp-sgd through the opacus library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae5190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Create & Train VAE -------- #\n",
    "\n",
    "# User defined hyperparams\n",
    "# General training\n",
    "batch_size = 32\n",
    "latent_dim = 8\n",
    "hidden_dim = 32\n",
    "n_epochs = 150\n",
    "logging_freq = 1  # Number of epochs we should log the results to the user\n",
    "patience = 50  # How many epochs should we allow the model train to see if\n",
    "# improvement is made\n",
    "delta = 10  # The difference between elbo values that registers an improvement\n",
    "filepath = None  # Where to save the best model\n",
    "\n",
    "\n",
    "# Privacy params\n",
    "differential_privacy = False  # Do we want to implement differential privacy\n",
    "sample_rate = 0.1  # Sampling rate\n",
    "C = 1e16  # Clipping threshold - any gradients above this are clipped\n",
    "noise_scale = None  # Noise multiplier - influences how much noise to add\n",
    "target_eps = 1  # Target epsilon for privacy accountant\n",
    "target_delta = 1e-5  # Target delta for privacy accountant\n",
    "\n",
    "# Prepare data for interaction with torch VAE\n",
    "Y = torch.Tensor(x_train)\n",
    "dataset = TensorDataset(Y)\n",
    "\n",
    "generator = None\n",
    "sample_rate = batch_size / len(dataset)\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_sampler=UniformWithReplacementSampler(\n",
    "        num_samples=len(dataset), sample_rate=sample_rate, generator=generator\n",
    "    ),\n",
    "    pin_memory=True,\n",
    "    generator=generator,\n",
    ")\n",
    "\n",
    "# Create VAE\n",
    "encoder = Encoder(x_train.shape[1], latent_dim, hidden_dim=hidden_dim)\n",
    "decoder = Decoder(latent_dim, num_continuous, num_categories=num_categories)\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "print(vae)\n",
    "\n",
    "if differential_privacy == False:\n",
    "    (\n",
    "        training_epochs,\n",
    "        log_elbo,\n",
    "        log_reconstruction,\n",
    "        log_divergence,\n",
    "        log_categorical,\n",
    "        log_numerical,\n",
    "    ) = vae.train(\n",
    "        data_loader, \n",
    "        n_epochs=n_epochs,\n",
    "        logging_freq=logging_freq,\n",
    "        patience=patience,\n",
    "        delta=delta,\n",
    "    )\n",
    "\n",
    "elif differential_privacy == True:\n",
    "    (\n",
    "        training_epochs,\n",
    "        log_elbo,\n",
    "        log_reconstruction,\n",
    "        log_divergence,\n",
    "        log_categorical,\n",
    "        log_numerical,\n",
    "    ) = vae.diff_priv_train(\n",
    "        data_loader,\n",
    "        n_epochs=n_epochs,\n",
    "        logging_freq=logging_freq,\n",
    "        patience=patience,\n",
    "        delta=delta,\n",
    "        C=C,\n",
    "        target_eps=target_eps,\n",
    "        target_delta=target_delta,\n",
    "        sample_rate=sample_rate,\n",
    "        noise_scale=noise_scale,\n",
    "    )\n",
    "    print(f\"(epsilon, delta): {vae.get_privacy_spent(target_delta)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c169e7",
   "metadata": {},
   "source": [
    "## Plotting Elbo Functionality\n",
    "\n",
    "Here we can plot and save the ELBO graph for the training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2208de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Plot Loss Features ELBO Breakdown -------- #\n",
    "\n",
    "elbo_fig = plot_elbo(\n",
    "    n_epochs=training_epochs,\n",
    "    log_elbo=log_elbo,\n",
    "    log_reconstruction=log_reconstruction,\n",
    "    log_divergence=log_divergence,\n",
    "    saving_filepath=\"\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626409ac",
   "metadata": {},
   "source": [
    "## Plotting Reconstruction Breakdown\n",
    "\n",
    "Here we can plot the breakdown of reconstruction loss i.e. visualise how the categorical and numerical losses change over training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ffee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Plot Loss Features Reconstruction Breakdown -------- #\n",
    "\n",
    "likelihood_fig = plot_likelihood_breakdown(\n",
    "    n_epochs=training_epochs,\n",
    "    log_categorical=log_categorical,\n",
    "    log_numerical=log_numerical,\n",
    "    saving_filepath=\"\",\n",
    "    pre_proc_method=pre_proc_method,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadfca69",
   "metadata": {},
   "source": [
    "## Synthetic Data Generation\n",
    "\n",
    "Here we create synthetic data ready for metric testing as well as visualisation of variable reconstruction. For this we simply generate from our generative model and then reverse transformations using the prior transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd8be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Synthetic Data Generation -------- #\n",
    "\n",
    "synthetic_sample = vae.generate(data_supp.shape[0])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    synthetic_sample = pd.DataFrame(\n",
    "        synthetic_sample.cpu().detach().numpy(), \n",
    "        columns=reordered_dataframe_columns\n",
    "    )\n",
    "else:\n",
    "    synthetic_sample = pd.DataFrame(\n",
    "        synthetic_sample.detach().numpy(), \n",
    "        columns=reordered_dataframe_columns\n",
    "    )\n",
    "\n",
    "# Reverse the transformations\n",
    "\n",
    "synthetic_supp = reverse_transformers(\n",
    "    synthetic_set=synthetic_sample,\n",
    "    data_supp_columns=data_supp.columns,\n",
    "    cont_transformers=continuous_transformers,\n",
    "    cat_transformers=categorical_transformers,\n",
    "    pre_proc_method=pre_proc_method,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7d8d8c",
   "metadata": {},
   "source": [
    "## Synthetic Variable Visualisation\n",
    "\n",
    "Here we want to visualise the synthetic variables generated and compare them to the original set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdaf4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Plot Histograms For All The Variable Distributions -------- #\n",
    "\n",
    "plot_variable_distributions(\n",
    "    categorical_columns=original_categorical_columns,\n",
    "    continuous_columns=original_continuous_columns,\n",
    "    data_supp=data_supp,\n",
    "    synthetic_supp=synthetic_supp,\n",
    "    saving_filepath=\"\",\n",
    "    pre_proc_method=pre_proc_method,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09630b17",
   "metadata": {},
   "source": [
    "## Metric evaluation\n",
    "\n",
    "We use the SDV evaluation framework. Supply the metrics you wish to find in the distributional_metrics list from SDV guidance. Can start here: https://sdv.dev/SDV/user_guides/evaluation/single_table_metrics.html\n",
    "\n",
    "Note that not all of these will work, some are hit and miss. We predominantly rely on continuous and discrete KL divergence measures. You can also input <b>\"gower\"</b> and this will calculate the gower distance using the gower library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f240e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- SDV Metrics -------- #\n",
    "\n",
    "# Define the metrics you want the model to evaluate\n",
    "\n",
    "# Define distributional metrics required - for sdv_baselines this is set by default\n",
    "distributional_metrics = [\n",
    "    \"SVCDetection\",\n",
    "    \"GMLogLikelihood\",\n",
    "    \"CSTest\",\n",
    "    \"KSTest\",\n",
    "    \"KSTestExtended\",\n",
    "    \"ContinuousKLDivergence\",\n",
    "    \"DiscreteKLDivergence\",\n",
    "]\n",
    "\n",
    "gower = False\n",
    "\n",
    "metrics = distribution_metrics(\n",
    "    gower_bool=gower,\n",
    "    distributional_metrics=distributional_metrics,\n",
    "    data_supp=data_supp,\n",
    "    synthetic_supp=synthetic_supp,\n",
    "    categorical_columns=original_categorical_columns,\n",
    "    continuous_columns=original_continuous_columns,\n",
    "    saving_filepath=None,\n",
    "    pre_proc_method=pre_proc_method,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc9065d",
   "metadata": {},
   "source": [
    "# Privacy Metric Evaluation\n",
    "\n",
    "Using SDV privacy metrics we can get an insight into how privacy is conserved when utilising dp-sgd methods. SDV's privacy metrics are limited in that they can only be used on similar data types. E.g. if we choose age to be the sensitive variably, we can build ML based models to predict a users age using the other columns. However we are forced to only use columns that are also continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de25de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify our private variable\n",
    "\n",
    "private_variable = \"x14\"\n",
    "\n",
    "privacy_metric = privacy_metrics(\n",
    "    private_variable=private_variable,\n",
    "    data_supp=data_supp,\n",
    "    synthetic_supp=synthetic_supp,\n",
    "    categorical_columns=original_categorical_columns,\n",
    "    continuous_columns=original_continuous_columns,\n",
    "    saving_filepath=None,\n",
    "    pre_proc_method=pre_proc_method,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48582f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717713d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
