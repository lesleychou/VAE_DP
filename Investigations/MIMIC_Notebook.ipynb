{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c082cd4d",
   "metadata": {},
   "source": [
    "# MIMIC Notebook\n",
    "\n",
    "This notebook runs through investigations on internal NHSX datasets collated from the MIMIC-III dataset.\n",
    "\n",
    "For users who do not have MIMIC-III access then investigations cannot be run through until access is completed. In the meantime you can access the investigations on the open access SUPPORT dataset.\n",
    "\n",
    "The notebook that produces our single table is found here <https://github.com/DaveBrind/SynthVAE/blob/main/MIMIC_preproc.ipynb>. If you want to create a single table yourself then follow the example csv file given at <https://github.com/DaveBrind/SynthVAE/blob/main/example_input.csv>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Import Libraries -------- #\n",
    "\n",
    "# Standard imports\n",
    "from tokenize import String\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# VAE is in other folder\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# Opacus support for differential privacy\n",
    "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
    "\n",
    "# For VAE dataset formatting\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# VAE functions\n",
    "from VAE import Decoder, Encoder, VAE\n",
    "\n",
    "# For datetime columns we need a transformer\n",
    "from rdt.transformers import datetime\n",
    "\n",
    "# Utility file contains all functions required to run notebook\n",
    "from utils import (\n",
    "    set_seed,\n",
    "    mimic_pre_proc,\n",
    "    constraint_filtering,\n",
    "    plot_elbo,\n",
    "    plot_likelihood_breakdown,\n",
    "    plot_variable_distributions,\n",
    "    reverse_transformers,\n",
    ")\n",
    "from metrics import distribution_metrics, privacy_metrics\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # We suppress warnings to avoid SDMETRICS throwing unique synthetic data warnings (i.e.\n",
    "# data in synthetic set is not in the real data set) as well as SKLEARN throwing convergence warnings (pre-processing uses\n",
    "# GMM from sklearn and this throws non convergence warnings)\n",
    "\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d8b521",
   "metadata": {},
   "source": [
    "## Data Loading & Column Definitions\n",
    "\n",
    "First we need to load in the MIMIC dataset from a specified filepath. \n",
    "\n",
    "We then need to create lists indicating which columns are:\n",
    "a) continuous\n",
    "b) categorical\n",
    "c) datetime\n",
    "\n",
    "Currently other data types are not supported. Importantly if columns contain missing data then they need to be dropped - Do not include these in original column lists & instead drop them from the loaded set in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8629429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the mimic single table data\n",
    "\n",
    "filepath = \"\"\n",
    "\n",
    "data_supp = pd.read_csv(filepath)\n",
    "original_categorical_columns = [\n",
    "    \"ETHNICITY\",\n",
    "    \"DISCHARGE_LOCATION\",\n",
    "    \"GENDER\",\n",
    "    \"FIRST_CAREUNIT\",\n",
    "    \"VALUEUOM\",\n",
    "    \"LABEL\",\n",
    "]\n",
    "original_continuous_columns = [\"SUBJECT_ID\", \"VALUE\", \"age\"]\n",
    "original_datetime_columns = [\"ADMITTIME\", \"DISCHTIME\", \"DOB\", \"CHARTTIME\"]\n",
    "\n",
    "# Drop DOD column as it contains NANS - for now\n",
    "\n",
    "# data_supp = data_supp.drop('DOD', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ddb42",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "\n",
    "Data can be pre-processed in 2 ways. Either we use <b>\"standard\"</b> option which performs a standard scaler on continuous variables - This has known limitations as:\n",
    "\n",
    "- Data in tables is usually non-gaussian and SynthVAE implements a gaussian loss, so this will perform worse unless the data is KNOWN to follow a gaussian distribution already.\n",
    "\n",
    "Or we use the second option of <b>\"GMM\"</b>. This performs a variational gaussian mixture model to scale the data & transform it to a gaussian distribution. We use a maximum number of clusters of 10 but the variational method will select the best number of clusters for that continuous variable. This also has known limitations:\n",
    "\n",
    "- 10 Clusters is arbitrary and may not be enough for certain variables.\n",
    "- We are fitting a model to transform the data and hence we are approximating before model is trained. This will lose fidelity as the distribution will not be transformed perfectly.\n",
    "\n",
    "\n",
    "For datasets that include datetime columns, original_metric_set returns the initial dataset after these columns have been transformed. This is because:\n",
    "\n",
    "- Our evaluation suite cannot calculate certain metrics on datetime objects so these need to be converted to continuous values first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13839178",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_proc_method = \"standard\"  # Select pre-processing method standard or GMM\n",
    "\n",
    "#%% -------- Data Pre-Processing -------- #\n",
    "\n",
    "(\n",
    "    x_train,\n",
    "    original_metric_set,\n",
    "    reordered_dataframe_columns,\n",
    "    continuous_transformers,\n",
    "    categorical_transformers,\n",
    "    datetime_transformers,\n",
    "    num_categories,\n",
    "    num_continuous,\n",
    ") = mimic_pre_proc(data_supp=data_supp, pre_proc_method=pre_proc_method)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c705de",
   "metadata": {},
   "source": [
    "## Creation & Training of VAE\n",
    "\n",
    "We can adapt certain parameters of the model e.g. batch size, latent dimension size etc. This model implements early stopping and these values can be adapted.\n",
    "\n",
    "We can also activate differential privacy by implementing dp-sgd through the opacus library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58de105",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Create & Train VAE -------- #\n",
    "\n",
    "# User defined hyperparams\n",
    "# General training\n",
    "batch_size = 32\n",
    "latent_dim = 256\n",
    "hidden_dim = 256\n",
    "n_epochs = 5\n",
    "logging_freq = 1  # Number of epochs we should log the results to the user\n",
    "patience = 5  # How many epochs should we allow the model train to see if\n",
    "# improvement is made\n",
    "delta = 10  # The difference between elbo values that registers an improvement\n",
    "filepath = None  # Where to save the best model\n",
    "\n",
    "\n",
    "# Privacy params\n",
    "differential_privacy = False  # Do we want to implement differential privacy\n",
    "sample_rate = 0.1  # Sampling rate\n",
    "C = 1e16  # Clipping threshold - any gradients above this are clipped\n",
    "noise_scale = None  # Noise multiplier - influences how much noise to add\n",
    "target_eps = 1  # Target epsilon for privacy accountant\n",
    "target_delta = 1e-5  # Target delta for privacy accountant\n",
    "\n",
    "\n",
    "# Prepare data for interaction with torch VAE\n",
    "Y = torch.Tensor(x_train)\n",
    "dataset = TensorDataset(Y)\n",
    "\n",
    "generator = None\n",
    "sample_rate = batch_size / len(dataset)\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_sampler=UniformWithReplacementSampler(\n",
    "        num_samples=len(dataset), sample_rate=sample_rate, generator=generator\n",
    "    ),\n",
    "    pin_memory=True,\n",
    "    generator=generator,\n",
    ")\n",
    "\n",
    "# Create VAE\n",
    "\n",
    "encoder = Encoder(x_train.shape[1], latent_dim, hidden_dim=hidden_dim)\n",
    "decoder = Decoder(latent_dim, num_continuous, num_categories=num_categories)\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "print(vae)\n",
    "\n",
    "if differential_privacy == False:\n",
    "    (\n",
    "        training_epochs,\n",
    "        log_elbo,\n",
    "        log_reconstruction,\n",
    "        log_divergence,\n",
    "        log_categorical,\n",
    "        log_numerical,\n",
    "    ) = vae.train(\n",
    "        data_loader, \n",
    "        n_epochs=n_epochs,\n",
    "        logging_freq=logging_freq,\n",
    "        patience=patience,\n",
    "        delta=delta,\n",
    "    )\n",
    "\n",
    "elif differential_privacy == True:\n",
    "    (\n",
    "        training_epochs,\n",
    "        log_elbo,\n",
    "        log_reconstruction,\n",
    "        log_divergence,\n",
    "        log_categorical,\n",
    "        log_numerical,\n",
    "    ) = vae.diff_priv_train(\n",
    "        data_loader,\n",
    "        n_epochs=n_epochs,\n",
    "        logging_freq=logging_freq,\n",
    "        patience=patience,\n",
    "        delta=delta,\n",
    "        C=C,\n",
    "        target_eps=target_eps,\n",
    "        target_delta=target_delta,\n",
    "        sample_rate=sample_rate,\n",
    "        noise_scale=noise_scale,\n",
    "    )\n",
    "    print(f\"(epsilon, delta): {vae.get_privacy_spent(target_delta)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1110e951",
   "metadata": {},
   "source": [
    "## Plotting Elbo Functionality\n",
    "\n",
    "Here we can plot and save the ELBO graph for the training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1474f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Plot Loss Features ELBO Breakdown -------- #\n",
    "\n",
    "elbo_fig = plot_elbo(\n",
    "    n_epochs=training_epochs,\n",
    "    log_elbo=log_elbo,\n",
    "    log_reconstruction=log_reconstruction,\n",
    "    log_divergence=log_divergence,\n",
    "    saving_filepath=\"\",\n",
    "    pre_proc_method=pre_proc_method,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106e3548",
   "metadata": {},
   "source": [
    "## Plotting Reconstruction Breakdown\n",
    "\n",
    "Here we can plot the breakdown of reconstruction loss i.e. visualise how the categorical and numerical losses change over training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4deaa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Plot Loss Features Reconstruction Breakdown -------- #\n",
    "\n",
    "likelihood_fig = plot_likelihood_breakdown(\n",
    "    n_epochs=training_epochs,\n",
    "    log_categorical=log_categorical,\n",
    "    log_numerical=log_numerical,\n",
    "    saving_filepath=\"\",\n",
    "    pre_proc_method=pre_proc_method,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ae4717",
   "metadata": {},
   "source": [
    "## Synthetic Data Generation\n",
    "\n",
    "Here we create synthetic data ready for metric testing as well as visualisation of variable reconstruction.\n",
    "\n",
    "If you are using the MIMIC-III internal set from NHSX then constraint sampling here checks to ensure certain constraints are obeyed in the synthetic set. These are:\n",
    "\n",
    "- age is greater than or equal to 0\n",
    "- The admission date is after the date of birth\n",
    "- The discharge date is after or equal to the admission date\n",
    "- The first chart time is also after or equal to admission date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d88f35",
   "metadata": {},
   "source": [
    "## Either run the cell directly below for constraints included in sampling OR run the cell second below to just generate a sample without constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36881d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Constraint Sampling -------- #\n",
    "\n",
    "# For NHSX internal MIMIC set OR sets which follow a similar data structure\n",
    "\n",
    "synthetic_supp = constraint_filtering(\n",
    "    n_rows=data_supp.shape[0],\n",
    "    vae=vae,\n",
    "    reordered_cols=reordered_dataframe_columns,\n",
    "    data_supp_columns=data_supp.columns,\n",
    "    cont_transformers=continuous_transformers,\n",
    "    cat_transformers=categorical_transformers,\n",
    "    date_transformers=datetime_transformers,\n",
    "    pre_proc_method=pre_proc_method,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b1dbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Synthetic Data Generation Without Constraints -------- #\n",
    "\n",
    "# For any other datasets OR for running without constraint sampling\n",
    "\n",
    "synthetic_sample = vae.generate(data_supp.shape[0])\n",
    "\n",
    "# Reverse the transformations\n",
    "\n",
    "synthetic_supp = reverse_transformers(\n",
    "    synthetic_set=synthetic_sample,\n",
    "    data_supp_columns=data_supp.columns,\n",
    "    cont_transformers=continuous_transformers,\n",
    "    cat_transformers=categorical_transformers,\n",
    "    date_transformers=datetime_transformers,\n",
    "    pre_proc_method=pre_proc_method,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33445ad7",
   "metadata": {},
   "source": [
    "## Synthetic Variable Visualisation\n",
    "\n",
    "Here we want to visualise the synthetic variables generated and compare them to the original set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9511a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Plot Histograms For All The Variable Distributions -------- #\n",
    "\n",
    "plot_variable_distributions(\n",
    "    categorical_columns=original_categorical_columns,\n",
    "    continuous_columns=original_continuous_columns,\n",
    "    data_supp=data_supp,\n",
    "    synthetic_supp=synthetic_supp,\n",
    "    saving_filepath=\"\",\n",
    "    pre_proc_method=pre_proc_method,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f1ebb0",
   "metadata": {},
   "source": [
    "## Metric evaluation\n",
    "\n",
    "For datasets that have datetime columns, we need to re-transform these into a numerical value as our metrics cannot handle datetime objects. We are then inputting <b>original_metric_set</b> alongside the newly transformed synthetic set i.e. <b>metric_synthetic_supp</b>. If datetimes are not included in the set then you can just run <b>data_supp</b> against <b>synthetic_supp</b> and skip the datetime handling.\n",
    "\n",
    "We use the SDV evaluation framework. Supply the metrics you wish to find in the user_metrics list from SDV guidance. Can start here: https://sdv.dev/SDV/user_guides/evaluation/single_table_metrics.html\n",
    "\n",
    "Note that not all of these will work, some are hit and miss. We predominantly rely on continuous and discrete KL divergence measures. You can also input <b>\"gower\"</b> and this will calculate the gower distance using the gower library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d845b202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Datetime Handling -------- #\n",
    "\n",
    "# If the dataset has datetimes then we need to re-convert these to a numerical\n",
    "# Value representing seconds, this is so we can evaluate the metrics on them\n",
    "\n",
    "metric_synthetic_supp = synthetic_supp.copy()\n",
    "\n",
    "for index, column in enumerate(original_datetime_columns):\n",
    "\n",
    "    # Fit datetime transformer - converts to seconds\n",
    "    temp_datetime = datetime.DatetimeTransformer()\n",
    "    temp_datetime.fit(metric_synthetic_supp, columns=column)\n",
    "\n",
    "    metric_synthetic_supp = temp_datetime.transform(metric_synthetic_supp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e4174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- SDV Metrics -------- #\n",
    "\n",
    "# Define the metrics you want the model to evaluate\n",
    "\n",
    "# Define distributional metrics required - for sdv_baselines this is set by default\n",
    "distributional_metrics = [\n",
    "    \"SVCDetection\",\n",
    "    \"GMLogLikelihood\",\n",
    "    \"CSTest\",\n",
    "    \"KSTest\",\n",
    "    \"KSTestExtended\",\n",
    "    \"ContinuousKLDivergence\",\n",
    "    \"DiscreteKLDivergence\",\n",
    "]\n",
    "\n",
    "gower = False\n",
    "\n",
    "metrics = distribution_metrics(\n",
    "    gower_bool=gower,\n",
    "    distributional_metrics=distributional_metrics,\n",
    "    data_supp=original_metric_set,\n",
    "    synthetic_supp=metric_synthetic_supp,\n",
    "    categorical_columns=original_categorical_columns,\n",
    "    continuous_columns=original_continuous_columns,\n",
    "    saving_filepath=\"\",\n",
    "    pre_proc_method=pre_proc_method,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8bba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975c5d80",
   "metadata": {},
   "source": [
    "# Privacy Metric Evaluation\n",
    "\n",
    "Using SDV privacy metrics we can get an insight into how privacy is conserved when utilising dp-sgd methods. SDV's privacy metrics are limited in that they can only be used on similar data types. E.g. if we choose age to be the sensitive variable, we can build ML based models to predict a users age using the other columns. However we are forced to only use columns that are also continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4c9495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify our private variable\n",
    "\n",
    "private_variable = \"ETHNICITY\"\n",
    "\n",
    "privacy_metric = privacy_metrics(\n",
    "    private_variable=private_variable,\n",
    "    data_supp=data_supp,\n",
    "    synthetic_supp=synthetic_supp,\n",
    "    categorical_columns=original_categorical_columns,\n",
    "    continuous_columns=original_continuous_columns,\n",
    "    saving_filepath=None,\n",
    "    pre_proc_method=pre_proc_method,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b74b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_metric"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
